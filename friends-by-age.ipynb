{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Will</th>\n",
       "      <th>33</th>\n",
       "      <th>385</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Jean-Luc</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Hugh</td>\n",
       "      <td>55</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Deanna</td>\n",
       "      <td>40</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Quark</td>\n",
       "      <td>68</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Weyoun</td>\n",
       "      <td>59</td>\n",
       "      <td>318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      Will  33  385\n",
       "0  1  Jean-Luc  26    2\n",
       "1  2      Hugh  55  221\n",
       "2  3    Deanna  40  465\n",
       "3  4     Quark  68   21\n",
       "4  5    Weyoun  59  318"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "file = pd.read_csv(r\"C:\\DataScience\\Jupyter Files\\Spark\\Datasets\\fakefriends.csv\")\n",
    "file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the first column is the index, the second column is the name, the third column is the age, and the fourth column is the number of friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0,Will,33,385', '1,Jean-Luc,26,2', '2,Hugh,55,221', '3,Deanna,40,465', '4,Quark,68,21']\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(r\"C:\\DataScience\\Jupyter Files\\Spark\\Datasets\\fakefriends.csv\")\n",
    "print(lines.take(5))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row of the RDD contains information splited by ','. In order to extract each information, we need to split the row by ','."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33, 385), (26, 2), (55, 221), (40, 465), (68, 21)]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(r\"C:\\DataScience\\Jupyter Files\\Spark\\Datasets\\fakefriends.csv\")\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "rdd = lines.map(parseLine)\n",
    "print(rdd.take(5))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have tuples where the first element is the age, and the second element is the number of friends. Notice that in Spark, the first element is the index, and the second element is the value. We can then combine all rows based on the index (age)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33, (385, 1)), (26, (2, 1)), (55, (221, 1)), (40, (465, 1)), (68, (21, 1))]\n",
      "[((33, 385), 1), ((26, 2), 1), ((55, 221), 1), ((40, 465), 1), ((68, 21), 1)]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(r\"C:\\DataScience\\Jupyter Files\\Spark\\Datasets\\fakefriends.csv\")\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x:(x, 1))\n",
    "totalsByAge_map = rdd.map(lambda x: (x, 1))\n",
    "print(totalsByAge.take(5))\n",
    "print(totalsByAge_map.take(5))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we convert the number of friends to a tuple (number of friends, 1), so that later we can count the frequency.\n",
    "Different between **map()** and **mapValues**:\n",
    "- map() applys function to both the index and the value.\n",
    "- mapValues() applys function only to the value.\n",
    "\n",
    "**Note that here age is the index.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33, (3904, 12)), (26, (4115, 17)), (55, (3842, 13)), (40, (4264, 17)), (68, (2696, 10))]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(r\"C:\\DataScience\\Jupyter Files\\Spark\\Datasets\\fakefriends.csv\")\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x:(x, 1))\n",
    "totalsByAge = totalsByAge.reduceByKey(lambda x1, x2: (x1[0] + x2[0], x1[1] + x2[1]))\n",
    "print(totalsByAge.take(5))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use **reduceByKey()** method to merge all rows with the same index (age), and add both the number of friends and the frequencies together. Notice for **reduceByKey()**, the function has two variables: the first variable is what we have so far, and the second variable is the new value that's gonna be merged. You can think the first variable as the *sum*, while the second variable is the *increment*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(33, 325.3333333333333), (26, 242.05882352941177), (55, 295.53846153846155), (40, 250.8235294117647), (68, 269.6)]\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(r\"C:\\DataScience\\Jupyter Files\\Spark\\Datasets\\fakefriends.csv\")\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x:(x, 1))\n",
    "totalsByAge = totalsByAge.reduceByKey(lambda x1, x2: (x1[0] + x2[0], x1[1] + x2[1]))\n",
    "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
    "print(averagesByAge.take(5))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just calculate the average number of friends by dividing the total number of friends by the total frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 325.3333333333333)\n",
      "(26, 242.05882352941177)\n",
      "(55, 295.53846153846155)\n",
      "(40, 250.8235294117647)\n",
      "(68, 269.6)\n",
      "(59, 220.0)\n",
      "(37, 249.33333333333334)\n",
      "(54, 278.0769230769231)\n",
      "(38, 193.53333333333333)\n",
      "(27, 228.125)\n",
      "(53, 222.85714285714286)\n",
      "(57, 258.8333333333333)\n",
      "(56, 306.6666666666667)\n",
      "(43, 230.57142857142858)\n",
      "(36, 246.6)\n",
      "(22, 206.42857142857142)\n",
      "(35, 211.625)\n",
      "(45, 309.53846153846155)\n",
      "(60, 202.71428571428572)\n",
      "(67, 214.625)\n",
      "(19, 213.27272727272728)\n",
      "(30, 235.8181818181818)\n",
      "(51, 302.14285714285717)\n",
      "(25, 197.45454545454547)\n",
      "(21, 350.875)\n",
      "(42, 303.5)\n",
      "(49, 184.66666666666666)\n",
      "(48, 281.4)\n",
      "(50, 254.6)\n",
      "(39, 169.28571428571428)\n",
      "(32, 207.9090909090909)\n",
      "(58, 116.54545454545455)\n",
      "(64, 281.3333333333333)\n",
      "(31, 267.25)\n",
      "(52, 340.6363636363636)\n",
      "(24, 233.8)\n",
      "(20, 165.0)\n",
      "(62, 220.76923076923077)\n",
      "(41, 268.55555555555554)\n",
      "(44, 282.1666666666667)\n",
      "(69, 235.2)\n",
      "(65, 298.2)\n",
      "(61, 256.22222222222223)\n",
      "(28, 209.1)\n",
      "(66, 276.44444444444446)\n",
      "(46, 223.69230769230768)\n",
      "(29, 215.91666666666666)\n",
      "(18, 343.375)\n",
      "(47, 233.22222222222223)\n",
      "(34, 245.5)\n",
      "(63, 384.0)\n",
      "(23, 246.3)\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"FriendsByAge\")\n",
    "sc = SparkContext(conf = conf)\n",
    "\n",
    "lines = sc.textFile(r\"C:\\DataScience\\Jupyter Files\\Spark\\Datasets\\fakefriends.csv\")\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    age = int(fields[2])\n",
    "    numFriends = int(fields[3])\n",
    "    return (age, numFriends)\n",
    "\n",
    "rdd = lines.map(parseLine)\n",
    "totalsByAge = rdd.mapValues(lambda x:(x, 1))\n",
    "totalsByAge = totalsByAge.reduceByKey(lambda x1, x2: (x1[0] + x2[0], x1[1] + x2[1]))\n",
    "averagesByAge = totalsByAge.mapValues(lambda x: x[0] / x[1])\n",
    "results = averagesByAge.collect()\n",
    "for result in results:\n",
    "    print(result)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect()**: return all the results as an array. Now the results are not in the form of RDD. Collect is one of the RDD actions. The RDD actions are functions that convert the RDD to some results. Before the actions, you may have multiple RDD transforming, **but noting actually happens until an action is called!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
